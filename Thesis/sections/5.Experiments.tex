% !TEX root = ..\main.tex
\chapter{Experiments}\label{ch:Experiments}

\section{Experiment 1: Validation of the PoC}
The goal of this experiment is to provide validation of the implementation in the PoC. This is done with the results of the paper \textit{"A Comprehensive Study of Bloated Dependencies in the Maven Ecosystem"} by Soto-Valero et al. \cite{soto2020comprehensive}. In this work, a byte-code analysis is performed on Maven Artifacts to detect which of the dependencies of this artifacts are \textit{bloated}. Although the PoC created in this thesis does not perform exactly the same kind of analysis, we consider that a dependency is \textit{bloated} if all the metrics have value zero for that dependency.

\subsection{Experimental set up}
Soto-Valero et al. perform a qualitative analysis, in which they analyze 31 libraries available as Maven artifacts. If \textit{bloated} dependencies are found during the analysis, a \textit{Pull Request} is made to the \textit{GitHub} repository of the artifact in which the \textit{bloated} dependencies are deleted from the \textit{pom} file.

With the information in the paper, we collect the \textit{GroupId} and \textit{ArtifactId} of the 31 artifacts. Out of the 31, 2 could not be found in the \textit{Maven Repository Central}. For the other 29, the \textit{version} to use in the experiment is determined by finding the last version released before the experiment by Soto-Valero et al. was conducted - November of 2019.

Of the 29 artifacts, X\todo{determine how many} could not be used with the PoC, because either the artifact itself or some dependency could not be downloaded from \textit{Maven Central}, leaving a set of X \todo{find out the number} libraries.

% MDD: https://data.4tu.nl/articles/dataset/The_Maven_Dependency_Dataset/12698027
\subsection{Execution}

- New request set up

- structure of the file in the request

- structure of the result of the comparison

\subsection{Results}

- how many libraries have different results and why.

- overall evaluation

\section{Experiment 2: Validation of the direct metrics}

Goal of the experiment: collect some real world data of the impact of a dependency in a project, and compare it with the metrics calculated.


\subsection{Experimental set up}
- Explain what is the MDD, and which schema it uses

- Create new table with all the breaking changes, and the information about the two versions of the library, the previous to the change and the one that had the breaking change.

- Create new table based on the table dependencies. Table containing a library (client) identified by group and artifact, for which two different versions of the library have a dependency with another library (server), with same artifact and group but different versions.

- Match the server library of the second table with the libraries with breaking chages of the first table. This way, we have for each breaking change, the client libraries that have versions depdending on the version previous to the breaking change, and the version after the breaking change. This means that the client library has had to adapt to the breaking change, in case if was originally used.

- From the result table, it happens that more than one version of the client library can depend on the same version of the server library, both previous and posterior to the change. That is why, a new filtered table is created, conatining for each breaking change, only the last version of the client library depending on the version previous to the change, and the first version of the client library depending on the version with the change.

- Also, I filter out all the dependencies that belong to the same group, since the developments of these dependencies are probably synchronized, and the impact works differently (cite counting those that matter)

\subsection{Execution}
\subsection{Results}


\reminder{The same dataset and strategy is going to be used for the transitive metrics. And, it could also be used for the percentage of usage. I have to develop a bit more on this}

\section{Experiment 3: Validation of visualization}


\subsection{Experimental set up}


- Explain which stage of the visualization design is being validated, and why is domain expert interview a good idea.

- Describe the different type of questions (topic, and open question or scaled...)

- Explain the set up of the interviews (online, control to the interviewee when using the tool...)

\subsection{Execution}
\subsection{Results}
